{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM, Dense, Embedding, Input, Dropout, Activation, GlobalMaxPool1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = 'C://Users//H//Desktop//Spring18//-AdvancedDatabases//Project//Project_Work//final_binary_dataset.csv'\n",
    "data = pd.read_csv(dataset, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>For guns?</th>\n",
       "      <th>Location</th>\n",
       "      <th>For guns</th>\n",
       "      <th>Against guns</th>\n",
       "      <th>Makes no sense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@Mary_rnntt2 @lauren_hoggs You're blinded by y...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Mountain Lakes, NJ</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey #MNLEG, don’t be idiotic. gun laws, not pa...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Minneapolis, MN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gun owners, women, kids, all opposed to the ab...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@PhilipRucker Is he taking guns away from ment...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Broomall, PA</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Blissfully unaware that it’s too late: Califor...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Teaneck, NJ</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  For guns?  \\\n",
       "0  @Mary_rnntt2 @lauren_hoggs You're blinded by y...        0.0   \n",
       "1  Hey #MNLEG, don’t be idiotic. gun laws, not pa...        0.0   \n",
       "2  Gun owners, women, kids, all opposed to the ab...        0.0   \n",
       "3  @PhilipRucker Is he taking guns away from ment...        0.0   \n",
       "4  Blissfully unaware that it’s too late: Califor...        1.0   \n",
       "\n",
       "             Location  For guns  Against guns  Makes no sense  \n",
       "0  Mountain Lakes, NJ         0             1               0  \n",
       "1     Minneapolis, MN         0             1               0  \n",
       "2         Houston, TX         0             1               0  \n",
       "3        Broomall, PA         0             1               0  \n",
       "4         Teaneck, NJ         1             0               0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training = data.iloc[:200]\n",
    "testing = data.iloc[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['For guns', 'Against guns']\n",
    "features_train = training[labels].values\n",
    "features_test = testing[labels].values\n",
    "tweet_train = training['Tweet']\n",
    "tweet_test = testing['Tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "chunk_size = 200\n",
    "\n",
    "def convert_embedding(text):\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(list(text))\n",
    "    tokenized = tokenizer.texts_to_sequences(text)\n",
    "    embedding = pad_sequences(tokenized, maxlen=chunk_size)   # Every Tweet is represented by vector of length 200\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = convert_embedding(tweet_train)\n",
    "x_test = convert_embedding(tweet_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building LSTM\n"
     ]
    }
   ],
   "source": [
    "print(\"Building LSTM\")\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM\n",
      "Train on 180 samples, validate on 20 samples\n",
      "Epoch 1/20\n",
      "180/180 [==============================] - 3s 18ms/step - loss: 0.6882 - acc: 0.6000 - val_loss: 0.6870 - val_acc: 0.6000\n",
      "Epoch 2/20\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.6651 - acc: 0.6500 - val_loss: 0.6723 - val_acc: 0.6000\n",
      "Epoch 3/20\n",
      "180/180 [==============================] - 2s 13ms/step - loss: 0.6273 - acc: 0.6500 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 4/20\n",
      "180/180 [==============================] - 3s 14ms/step - loss: 0.6017 - acc: 0.6500 - val_loss: 0.6722 - val_acc: 0.6000\n",
      "Epoch 5/20\n",
      "180/180 [==============================] - 3s 14ms/step - loss: 0.5536 - acc: 0.6611 - val_loss: 0.6884 - val_acc: 0.6000\n",
      "Epoch 6/20\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.4524 - acc: 0.7111 - val_loss: 0.6822 - val_acc: 0.6000\n",
      "Epoch 7/20\n",
      "180/180 [==============================] - 3s 15ms/step - loss: 0.3122 - acc: 0.9083 - val_loss: 0.6852 - val_acc: 0.5500\n",
      "Epoch 8/20\n",
      "180/180 [==============================] - 2s 13ms/step - loss: 0.1732 - acc: 0.9778 - val_loss: 1.1910 - val_acc: 0.6500\n",
      "Epoch 9/20\n",
      "180/180 [==============================] - 2s 13ms/step - loss: 0.1217 - acc: 0.9722 - val_loss: 0.7881 - val_acc: 0.3750\n",
      "Epoch 10/20\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.1290 - acc: 0.9611 - val_loss: 0.7635 - val_acc: 0.5000\n",
      "Epoch 11/20\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.1283 - acc: 0.9583 - val_loss: 1.4690 - val_acc: 0.6000\n",
      "Epoch 12/20\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.0813 - acc: 0.9722 - val_loss: 0.7543 - val_acc: 0.6000\n",
      "Epoch 13/20\n",
      "180/180 [==============================] - 3s 14ms/step - loss: 0.0640 - acc: 0.9889 - val_loss: 0.6585 - val_acc: 0.7000\n",
      "Epoch 14/20\n",
      "180/180 [==============================] - 2s 13ms/step - loss: 0.0481 - acc: 1.0000 - val_loss: 0.6626 - val_acc: 0.6500\n",
      "Epoch 15/20\n",
      "180/180 [==============================] - 3s 16ms/step - loss: 0.0392 - acc: 1.0000 - val_loss: 0.8648 - val_acc: 0.6000\n",
      "Epoch 16/20\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.0318 - acc: 0.9944 - val_loss: 1.2049 - val_acc: 0.6500\n",
      "Epoch 17/20\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.0590 - acc: 0.9944 - val_loss: 1.2263 - val_acc: 0.6000\n",
      "Epoch 18/20\n",
      "180/180 [==============================] - 3s 14ms/step - loss: 0.0493 - acc: 0.9944 - val_loss: 0.6881 - val_acc: 0.7000\n",
      "Epoch 19/20\n",
      "180/180 [==============================] - 2s 14ms/step - loss: 0.0235 - acc: 1.0000 - val_loss: 0.7153 - val_acc: 0.6000\n",
      "Epoch 20/20\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.0224 - acc: 1.0000 - val_loss: 0.6905 - val_acc: 0.7000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2099bdc6fd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training LSTM\")\n",
    "model.fit(x_train, features_train, batch_size=32, epochs=20, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 49 samples\n",
      "Epoch 1/5\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.6896 - acc: 0.6050 - val_loss: 0.6742 - val_acc: 0.7755\n",
      "Epoch 2/5\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.6782 - acc: 0.6450 - val_loss: 0.6463 - val_acc: 0.7755\n",
      "Epoch 3/5\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.6654 - acc: 0.6450 - val_loss: 0.6122 - val_acc: 0.7755\n",
      "Epoch 4/5\n",
      "200/200 [==============================] - 2s 9ms/step - loss: 0.6493 - acc: 0.6450 - val_loss: 0.5750 - val_acc: 0.7755\n",
      "Epoch 5/5\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.6403 - acc: 0.6450 - val_loss: 0.5634 - val_acc: 0.7755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x209ae67e860>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = Input(shape=(chunk_size, ))\n",
    "embed_size = 128\n",
    "x1 = Embedding(max_words, embed_size)(inp)\n",
    "x1 = LSTM(60, return_sequences=True, name='lstm_layer')(x1)\n",
    "x1 = GlobalMaxPool1D()(x1)\n",
    "x1 = Dropout(0.1)(x1)\n",
    "x1 = Dense(50, activation='relu')(x1)\n",
    "x1 = Dropout(0.1)(x1)\n",
    "x1 = Dense(2, activation='sigmoid')(x1)\n",
    "model = Model(inputs=inp, outputs=x1)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, features_train, epochs=5, batch_size=32, validation_data=(x_test, features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36695078, 0.6234031 ],\n",
       "       [0.36710042, 0.6242135 ],\n",
       "       [0.37757   , 0.61846733],\n",
       "       [0.37110582, 0.61395586],\n",
       "       [0.36575592, 0.6261747 ],\n",
       "       [0.36369658, 0.62905794],\n",
       "       [0.35492986, 0.63483834],\n",
       "       [0.35922006, 0.6315029 ],\n",
       "       [0.3679899 , 0.6238169 ],\n",
       "       [0.37044165, 0.6229391 ],\n",
       "       [0.3611755 , 0.6291321 ],\n",
       "       [0.37025034, 0.62524337],\n",
       "       [0.3687732 , 0.6268966 ],\n",
       "       [0.36466876, 0.6218073 ],\n",
       "       [0.36980283, 0.6124285 ],\n",
       "       [0.35718268, 0.6338981 ],\n",
       "       [0.36181366, 0.62929714],\n",
       "       [0.36944437, 0.62217516],\n",
       "       [0.36119252, 0.62435883],\n",
       "       [0.36447203, 0.6185644 ],\n",
       "       [0.3700261 , 0.619133  ],\n",
       "       [0.3584271 , 0.62629855],\n",
       "       [0.36620885, 0.6185269 ],\n",
       "       [0.35778505, 0.63377136],\n",
       "       [0.36542153, 0.61585975],\n",
       "       [0.3609792 , 0.62917817],\n",
       "       [0.36823344, 0.6222783 ],\n",
       "       [0.36551356, 0.6254545 ],\n",
       "       [0.3619303 , 0.6260158 ],\n",
       "       [0.36307952, 0.6269832 ],\n",
       "       [0.3740818 , 0.61383766],\n",
       "       [0.35720503, 0.6348032 ],\n",
       "       [0.36669564, 0.62151015],\n",
       "       [0.36413524, 0.62479997],\n",
       "       [0.3632048 , 0.6279178 ],\n",
       "       [0.36601207, 0.62793857],\n",
       "       [0.369125  , 0.62109613],\n",
       "       [0.36941248, 0.62190807],\n",
       "       [0.3711709 , 0.61352724],\n",
       "       [0.3640997 , 0.6292513 ],\n",
       "       [0.35978743, 0.63088024],\n",
       "       [0.3572052 , 0.63036865],\n",
       "       [0.36960635, 0.62281275],\n",
       "       [0.3641609 , 0.6257122 ],\n",
       "       [0.3726334 , 0.6193921 ],\n",
       "       [0.36672032, 0.6275661 ],\n",
       "       [0.36479694, 0.6262978 ],\n",
       "       [0.36785454, 0.620398  ],\n",
       "       [0.3676076 , 0.613705  ]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(x_test, batch_size=1024)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
